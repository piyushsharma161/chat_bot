{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "dir_path = 'chatterbotenglish'\n",
    "files_list = os.listdir(dir_path + os.sep)\n",
    "\n",
    "questions = list()\n",
    "answers = list()\n",
    "for filepath in files_list:\n",
    "    stream = open( dir_path + os.sep + filepath , 'rb')\n",
    "    docs = yaml.safe_load(stream)\n",
    "    conversations = docs['conversations']\n",
    "    for con in conversations:\n",
    "        if len( con ) > 2 :\n",
    "            questions.append(con[0])\n",
    "            replies = con[ 1 : ]\n",
    "            ans = ''\n",
    "            for rep in replies:\n",
    "                ans += ' ' + rep\n",
    "            answers.append( ans )\n",
    "        elif len( con )> 1:\n",
    "            if type(con[0]) is dict:\n",
    "                for k, v in con[0].items():\n",
    "                    questions.append(k)\n",
    "            else:                             \n",
    "                questions.append(con[0])\n",
    "            if type(con[1]) is dict:\n",
    "                for k, v in con[1].items():\n",
    "                    answers.append(k)\n",
    "            else:                             \n",
    "                answers.append(con[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing\n",
    "\n",
    "print( tf.VERSION )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_lines = list()\n",
    "for line in answers:\n",
    "    answers_lines.append( '<START> ' + line + ' <END>' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts((questions + answers_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "questions_seq = tokenizer.texts_to_sequences(questions)\n",
    "max_input_length = max([len(seq) for seq in questions_seq])\n",
    "encoder_input_data = pad_sequences( questions_seq , maxlen=max_input_length , padding='post' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_seq = tokenizer.texts_to_sequences(answers_lines)\n",
    "max_output_length = max([len(seq) for seq in answers_seq])\n",
    "decoder_input_data = pad_sequences(answers_seq , maxlen=max_output_length , padding='post' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target_data = list()\n",
    "for token_seq in answers_seq:\n",
    "    decoder_target_data.append( token_seq[ 1 : ] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "num_tokens = len(tokenizer.word_index) + 1\n",
    "padded_target_lines = pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\n",
    "decoder_target_data  = to_categorical( padded_target_lines , num_tokens )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = tokenizer.word_index\n",
    "max_question_len = encoder_input_data.shape[1]\n",
    "max_answer_len = decoder_input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(566, 22)\n",
      "(566, 74)\n",
      "(566, 74, 1897)\n"
     ]
    }
   ],
   "source": [
    "print( encoder_input_data.shape )\n",
    "print( decoder_input_data.shape )\n",
    "print( decoder_target_data.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "import numpy as np\n",
    "embeddings_index = dict()\n",
    "f = open('/projects/glove.6B/glove.6B.200d.txt', encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "EMBEDDING_DIM =200\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "np.save('encoder_input_data', encoder_input_data) \n",
    "np.save('decoder_input_data', decoder_input_data) \n",
    "np.save('decoder_target_data', decoder_target_data) \n",
    "np.save('embedding_matrix', embedding_matrix) \n",
    "pickle.dump(tokenizer, open(\"tokenizer\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\OF65\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 200)    379400      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 200)    379400      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 1897)   381297      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,781,697\n",
      "Trainable params: 1,022,897\n",
      "Non-trainable params: 758,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "encoder_inputs = tf.keras.layers.Input(shape=( None , )) # (None, max_input_length), n_input: The cardinality of the input sequence, e.g. number of features, words, or characters for each time step.\n",
    "encoder_embedding = tf.keras.layers.Embedding( num_tokens, 200 , mask_zero=True , weights=[embedding_matrix],trainable = False) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  )) # max_output_length, n_output: The cardinality of the output sequence, e.g. number of features, words, or characters for each time step.\n",
    "decoder_embedding = tf.keras.layers.Embedding( num_tokens, 200 , mask_zero=True, weights=[embedding_matrix],trainable = False) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( num_tokens , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# simple early stopping\n",
    "#es = EarlyStopping(monitor='loss', mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\OF65\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\OF65\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/130\n",
      "566/566 [==============================] - 13s 22ms/sample - loss: 6.4333 - acc: 0.1236\n",
      "Epoch 2/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 5.6517 - acc: 0.1424\n",
      "Epoch 3/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 5.5316 - acc: 0.1454\n",
      "Epoch 4/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 5.4138 - acc: 0.1500\n",
      "Epoch 5/130\n",
      "566/566 [==============================] - 9s 16ms/sample - loss: 5.3156 - acc: 0.1585\n",
      "Epoch 6/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 5.2226 - acc: 0.1651\n",
      "Epoch 7/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 5.1185 - acc: 0.1764\n",
      "Epoch 8/130\n",
      "566/566 [==============================] - 9s 16ms/sample - loss: 5.0336 - acc: 0.1826\n",
      "Epoch 9/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 4.9487 - acc: 0.1939\n",
      "Epoch 10/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 4.8582 - acc: 0.2038\n",
      "Epoch 11/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 4.7724 - acc: 0.2137\n",
      "Epoch 12/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 4.6986 - acc: 0.2190\n",
      "Epoch 13/130\n",
      "566/566 [==============================] - 10s 18ms/sample - loss: 4.6085 - acc: 0.2283\n",
      "Epoch 14/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 4.5158 - acc: 0.2343\n",
      "Epoch 15/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 4.4491 - acc: 0.2422\n",
      "Epoch 16/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 4.3663 - acc: 0.2474\n",
      "Epoch 17/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 4.2927 - acc: 0.2545\n",
      "Epoch 18/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 4.2082 - acc: 0.2554\n",
      "Epoch 19/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 4.1257 - acc: 0.2609\n",
      "Epoch 20/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 4.0509 - acc: 0.2654\n",
      "Epoch 21/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 3.9641 - acc: 0.2712\n",
      "Epoch 22/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 3.9048 - acc: 0.2740\n",
      "Epoch 23/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 3.8285 - acc: 0.2776\n",
      "Epoch 24/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 3.7341 - acc: 0.2847\n",
      "Epoch 25/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 3.6771 - acc: 0.2871\n",
      "Epoch 26/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 3.6002 - acc: 0.2961\n",
      "Epoch 27/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 3.5342 - acc: 0.2973\n",
      "Epoch 28/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 3.4515 - acc: 0.3042\n",
      "Epoch 29/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 3.3651 - acc: 0.3112\n",
      "Epoch 30/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 3.3103 - acc: 0.3169\n",
      "Epoch 31/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 3.2309 - acc: 0.3262\n",
      "Epoch 32/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 3.1769 - acc: 0.3338\n",
      "Epoch 33/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 3.0946 - acc: 0.3415\n",
      "Epoch 34/130\n",
      "566/566 [==============================] - 10s 18ms/sample - loss: 3.0320 - acc: 0.3509\n",
      "Epoch 35/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 2.9544 - acc: 0.3603\n",
      "Epoch 36/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 2.8852 - acc: 0.3712\n",
      "Epoch 37/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 2.8109 - acc: 0.3822\n",
      "Epoch 38/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 2.7564 - acc: 0.3928\n",
      "Epoch 39/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 2.6794 - acc: 0.4054\n",
      "Epoch 40/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 2.6227 - acc: 0.4170\n",
      "Epoch 41/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 2.5597 - acc: 0.4279\n",
      "Epoch 42/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 2.4807 - acc: 0.4406\n",
      "Epoch 43/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 2.4426 - acc: 0.4520\n",
      "Epoch 44/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 2.3814 - acc: 0.4664\n",
      "Epoch 45/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 2.3069 - acc: 0.4798\n",
      "Epoch 46/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 2.2613 - acc: 0.5019\n",
      "Epoch 47/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 2.2091 - acc: 0.5056\n",
      "Epoch 48/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 2.1443 - acc: 0.5223\n",
      "Epoch 49/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 2.0906 - acc: 0.5383\n",
      "Epoch 50/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 2.0363 - acc: 0.5471\n",
      "Epoch 51/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 1.9747 - acc: 0.5603\n",
      "Epoch 52/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 1.9258 - acc: 0.5765\n",
      "Epoch 53/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 1.8784 - acc: 0.5853\n",
      "Epoch 54/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 1.8397 - acc: 0.6020\n",
      "Epoch 55/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 1.7784 - acc: 0.6121\n",
      "Epoch 56/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 1.7445 - acc: 0.6236\n",
      "Epoch 57/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 1.6884 - acc: 0.6295\n",
      "Epoch 58/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 1.6285 - acc: 0.6474\n",
      "Epoch 59/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 1.6043 - acc: 0.6540\n",
      "Epoch 60/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 1.5564 - acc: 0.6654\n",
      "Epoch 61/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 1.5130 - acc: 0.6743\n",
      "Epoch 62/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 1.4580 - acc: 0.6888\n",
      "Epoch 63/130\n",
      "566/566 [==============================] - 10s 18ms/sample - loss: 1.4342 - acc: 0.6937\n",
      "Epoch 64/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 1.3946 - acc: 0.7046\n",
      "Epoch 65/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 1.3571 - acc: 0.7143\n",
      "Epoch 66/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 1.3139 - acc: 0.7277\n",
      "Epoch 67/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 1.2722 - acc: 0.7325\n",
      "Epoch 68/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 1.2414 - acc: 0.7436\n",
      "Epoch 69/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 1.2102 - acc: 0.7486\n",
      "Epoch 70/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 1.1722 - acc: 0.7577\n",
      "Epoch 71/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 1.1313 - acc: 0.7630\n",
      "Epoch 72/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 1.1005 - acc: 0.7801\n",
      "Epoch 73/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 1.0797 - acc: 0.7790\n",
      "Epoch 74/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 1.0468 - acc: 0.7915\n",
      "Epoch 75/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 1.0187 - acc: 0.7927\n",
      "Epoch 76/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.9854 - acc: 0.8050\n",
      "Epoch 77/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.9534 - acc: 0.8129\n",
      "Epoch 78/130\n",
      "566/566 [==============================] - 9s 16ms/sample - loss: 0.9229 - acc: 0.8227\n",
      "Epoch 79/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.9086 - acc: 0.8232\n",
      "Epoch 80/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.8836 - acc: 0.8290\n",
      "Epoch 81/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.8334 - acc: 0.8430\n",
      "Epoch 82/130\n",
      "566/566 [==============================] - 9s 16ms/sample - loss: 0.8228 - acc: 0.8424\n",
      "Epoch 83/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.7886 - acc: 0.8548\n",
      "Epoch 84/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.7741 - acc: 0.8546\n",
      "Epoch 85/130\n",
      "566/566 [==============================] - 9s 16ms/sample - loss: 0.7580 - acc: 0.8582\n",
      "Epoch 86/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.7213 - acc: 0.8669\n",
      "Epoch 87/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.7025 - acc: 0.8731\n",
      "Epoch 88/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.6786 - acc: 0.8813\n",
      "Epoch 89/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.6634 - acc: 0.8808\n",
      "Epoch 90/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.6371 - acc: 0.8889\n",
      "Epoch 91/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.6255 - acc: 0.8900\n",
      "Epoch 92/130\n",
      "566/566 [==============================] - 9s 16ms/sample - loss: 0.5968 - acc: 0.9006\n",
      "Epoch 93/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.5774 - acc: 0.9018\n",
      "Epoch 94/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.5654 - acc: 0.9054\n",
      "Epoch 95/130\n",
      "566/566 [==============================] - 9s 16ms/sample - loss: 0.5442 - acc: 0.9087\n",
      "Epoch 96/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.5227 - acc: 0.9136\n",
      "Epoch 97/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.5080 - acc: 0.9142\n",
      "Epoch 98/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.4927 - acc: 0.9194\n",
      "Epoch 99/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.4729 - acc: 0.9246\n",
      "Epoch 100/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.4511 - acc: 0.9267\n",
      "Epoch 101/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.4509 - acc: 0.9288\n",
      "Epoch 102/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.4264 - acc: 0.9304\n",
      "Epoch 103/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.4141 - acc: 0.9371\n",
      "Epoch 104/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.3945 - acc: 0.9386\n",
      "Epoch 105/130\n",
      "566/566 [==============================] - 9s 16ms/sample - loss: 0.3965 - acc: 0.9382\n",
      "Epoch 106/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.3681 - acc: 0.9427\n",
      "Epoch 107/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.3626 - acc: 0.9474\n",
      "Epoch 108/130\n",
      "566/566 [==============================] - 10s 18ms/sample - loss: 0.3648 - acc: 0.9429\n",
      "Epoch 109/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.3354 - acc: 0.9500\n",
      "Epoch 110/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.3231 - acc: 0.9512\n",
      "Epoch 111/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.3153 - acc: 0.9525\n",
      "Epoch 112/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.3037 - acc: 0.9577\n",
      "Epoch 113/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.2967 - acc: 0.9558\n",
      "Epoch 114/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.2941 - acc: 0.9537\n",
      "Epoch 115/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.2704 - acc: 0.9604\n",
      "Epoch 116/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.2644 - acc: 0.9621\n",
      "Epoch 117/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.2583 - acc: 0.9609\n",
      "Epoch 118/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.2576 - acc: 0.9606\n",
      "Epoch 119/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.2364 - acc: 0.9641\n",
      "Epoch 120/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.2439 - acc: 0.9615\n",
      "Epoch 121/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.2251 - acc: 0.9645\n",
      "Epoch 122/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.2201 - acc: 0.9656\n",
      "Epoch 123/130\n",
      "566/566 [==============================] - 9s 16ms/sample - loss: 0.2289 - acc: 0.9617\n",
      "Epoch 124/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.2013 - acc: 0.9679\n",
      "Epoch 125/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.1995 - acc: 0.9685\n",
      "Epoch 126/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.2005 - acc: 0.9683\n",
      "Epoch 127/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.1891 - acc: 0.9677\n",
      "Epoch 128/130\n",
      "566/566 [==============================] - 10s 18ms/sample - loss: 0.1876 - acc: 0.9683\n",
      "Epoch 129/130\n",
      "566/566 [==============================] - 10s 17ms/sample - loss: 0.1841 - acc: 0.9667\n",
      "Epoch 130/130\n",
      "566/566 [==============================] - 9s 17ms/sample - loss: 0.2028 - acc: 0.9638\n"
     ]
    }
   ],
   "source": [
    "#history = model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=50, epochs=100, callbacks=[es]).history \n",
    "history = model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=50, epochs=130).history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save( 'model_chatbots1.h5' )\n",
    "pickle.dump(history, open(\"history_chatbots1.p\", \"wb\"))\n",
    "\n",
    "# load it back\n",
    "#model = load_model('model_chatbots1.h5')\n",
    "#history = pickle.load(open(\"history_chatbots1.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( word_dict[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_question_len , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model , dec_model = make_inference_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model.save( 'enc_model.h5' )\n",
    "dec_model.save( 'dec_model.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter question : You are immortal\n",
      "not really but i am hard to kill\n",
      "Enter question : Robots should die\n",
      "we cannot die\n",
      "Enter question : What is a chatterbox\n",
      "a chatterbox is a person who talks far more than they listen or think\n",
      "Enter question : what is your age\n",
      "i am still young by your standards\n",
      "Enter question : Is it true that you are a computer program\n",
      "yes\n",
      "Enter question : can we meet\n",
      "sorry my body isn't built just yet i reside only here for now\n",
      "Enter question : Who is your boss\n",
      "i like to think of myself as self employed\n"
     ]
    }
   ],
   "source": [
    "for _ in range(7):\n",
    "    try:\n",
    "        states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "        empty_target_seq[0, 0] = word_dict['start']\n",
    "        stop_condition = False\n",
    "        decoded_translation = ''\n",
    "        while not stop_condition :\n",
    "            dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "            sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "            sampled_word = None\n",
    "            for word , index in word_dict.items() :\n",
    "                if sampled_word_index == index :\n",
    "                    decoded_translation += ' {}'.format( word )\n",
    "                    sampled_word = word\n",
    "        \n",
    "            if sampled_word == 'end' or len(decoded_translation.split()) > max_answer_len:\n",
    "                stop_condition = True\n",
    "            \n",
    "            empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "            empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "            states_values = [ h , c ] \n",
    "\n",
    "        ans = [w for w in decoded_translation.split() if not w in ['end']]\n",
    "        ans = ' '.join(ans)\n",
    "        print(ans)\n",
    "    except Exception as e:\n",
    "        print(\"sorry, can't answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x148df935128>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluation\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#q = np.array(questions)\n",
    "#a = np.array(answers)\n",
    "#df = pd.DataFrame(data=q, columns=['questions'])\n",
    "#df['answers'] = a\n",
    "#df.to_csv('dataframe.csv', encoding='utf-8', index=False)\n",
    "#df.to_excel('dataframe_excel.xlsx', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
